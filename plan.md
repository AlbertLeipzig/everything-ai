## Fundamentals (6 weeks?)

- Dataset
- ML
- Supervised / Unsupervised Learning → _Core framing for all ML tasks_
- LLM
- Attention Mechanism + Transformer => _Foundation of LLMs_
- NLP (Natural Language Processing )
- RAG (Retrieved-Augmented Generation)
- Embedding + Tokenization => _Necessary for NLP & RAG apps_
- Neural-Network => _Feed-forward, CNNs, RNNs_
- Backpropagation => _Core of training neural nets_
- Overfitting / Underfitting → _Practical problem you'll constantly debug_
- Gradient-Descent => _The engine that optimizes everything_
- Epoch

## Important (6 weeks?)

- Agent
- GAN-Generative Adversarial Network
- LLM-Large Language Model
- Prompt-Engineering
- RAG- Retrieval Augmented Generation
- Reinforcement-Learning

## Future

- Maths
- Structured Attention
